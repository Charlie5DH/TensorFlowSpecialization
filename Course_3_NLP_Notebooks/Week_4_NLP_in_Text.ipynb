{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Course 3 - Week 4 - Lesson 1 - Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX4Kg8DUTKWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s_k7oNCwdAq",
        "colab_type": "text"
      },
      "source": [
        "We've seen classification of text over the last few lessons. But what about if we want to generate new text. Now this might sound like new unbroken ground, but when you think about it, you've actually covered everything that you need to do this already. Instead of generating new text, how about thinking about it as a prediction problem. Remember when for example you had a bunch of pixels for a picture, and you trained a neural network to classify what those pixels were, and it would predict the contents of the image, like maybe a fashion item, or a piece of handwriting. Well, text prediction is very similar. We can get a body of texts, extract the full vocabulary from it, and then create datasets from that, where we make it phrase the Xs and the next word in that phrase to be the Ys. For example, consider the phrase, Twinkle, Twinkle, Little, Star. What if we were to create training data where the Xs are Twinkle, Twinkle, Little, and the Y is star. Then, whenever neural network sees the words Twinkle, Twinkle, Little, the predicted next word would be star. Thus given enough words in a corpus with a neural network trained on each of the phrases in that corpus, and the predicted next word, we can come up with some pretty sophisticated text generation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BOwsuGQQY9OL",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PRnDnCW-Z7qv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "10c5be72-4853-429e-cd95-439f47ad8b90"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "# this is an example of text. The entire song is contained into a single string\n",
        "data=\"In the town of Athy one Jeremy Lanigan \\n Battered away til he hadnt a pound. \\nHis father died and made him a man again \\n Left him a farm and ten acres of ground. \\nHe gave a grand party for friends and relations \\nWho didnt forget him when come to the wall, \\nAnd if youll but listen Ill make your eyes glisten \\nOf the rows and the ructions of Lanigans Ball. \\nMyself to be sure got free invitation, \\nFor all the nice girls and boys I might ask, \\nAnd just in a minute both friends and relations \\nWere dancing round merry as bees round a cask. \\nJudy ODaly, that nice little milliner, \\nShe tipped me a wink for to give her a call, \\nAnd I soon arrived with Peggy McGilligan \\nJust in time for Lanigans Ball. \\nThere were lashings of punch and wine for the ladies, \\nPotatoes and cakes; there was bacon and tea, \\nThere were the Nolans, Dolans, OGradys \\nCourting the girls and dancing away. \\nSongs they went round as plenty as water, \\nThe harp that once sounded in Taras old hall,\\nSweet Nelly Gray and The Rat Catchers Daughter,\\nAll singing together at Lanigans Ball. \\nThey were doing all kinds of nonsensical polkas \\nAll round the room in a whirligig. \\nJulia and I, we banished their nonsense \\nAnd tipped them the twist of a reel and a jig. \\nAch mavrone, how the girls got all mad at me \\nDanced til youd think the ceiling would fall. \\nFor I spent three weeks at Brooks Academy \\nLearning new steps for Lanigans Ball. \\nThree long weeks I spent up in Dublin, \\nThree long weeks to learn nothing at all,\\n Three long weeks I spent up in Dublin, \\nLearning new steps for Lanigans Ball. \\nShe stepped out and I stepped in again, \\nI stepped out and she stepped in again, \\nShe stepped out and I stepped in again, \\nLearning new steps for Lanigans Ball. \\nBoys were all merry and the girls they were hearty \\nAnd danced all around in couples and groups, \\nTil an accident happened, young Terrance McCarthy \\nPut his right leg through miss Finnertys hoops. \\nPoor creature fainted and cried Meelia murther, \\nCalled for her brothers and gathered them all. \\nCarmody swore that hed go no further \\nTil he had satisfaction at Lanigans Ball. \\nIn the midst of the row miss Kerrigan fainted, \\nHer cheeks at the same time as red as a rose. \\nSome of the lads declared she was painted, \\nShe took a small drop too much, I suppose. \\nHer sweetheart, Ned Morgan, so powerful and able, \\nWhen he saw his fair colleen stretched out by the wall, \\nTore the left leg from under the table \\nAnd smashed all the Chaneys at Lanigans Ball. \\nBoys, oh boys, twas then there were runctions. \\nMyself got a lick from big Phelim McHugh. \\nI soon replied to his introduction \\nAnd kicked up a terrible hullabaloo. \\nOld Casey, the piper, was near being strangled. \\nThey squeezed up his pipes, bellows, chanters and all. \\nThe girls, in their ribbons, they got all entangled \\nAnd that put an end to Lanigans Ball.\"\n",
        "\n",
        "# caling split we can create a python list of sentences\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "# fit the tokenizer and create a dictionary of words/tokens\n",
        "# key/value pair with the key being the word and the value the token\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "# total number of words in dictionary +1 for oov words\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'and': 1, 'the': 2, 'a': 3, 'in': 4, 'all': 5, 'i': 6, 'for': 7, 'of': 8, 'lanigans': 9, 'ball': 10, 'were': 11, 'at': 12, 'to': 13, 'she': 14, 'stepped': 15, 'his': 16, 'girls': 17, 'as': 18, 'they': 19, 'til': 20, 'he': 21, 'again': 22, 'got': 23, 'boys': 24, 'round': 25, 'that': 26, 'her': 27, 'there': 28, 'three': 29, 'weeks': 30, 'up': 31, 'out': 32, 'him': 33, 'was': 34, 'spent': 35, 'learning': 36, 'new': 37, 'steps': 38, 'long': 39, 'away': 40, 'left': 41, 'friends': 42, 'relations': 43, 'when': 44, 'wall': 45, 'myself': 46, 'nice': 47, 'just': 48, 'dancing': 49, 'merry': 50, 'tipped': 51, 'me': 52, 'soon': 53, 'time': 54, 'old': 55, 'their': 56, 'them': 57, 'danced': 58, 'dublin': 59, 'an': 60, 'put': 61, 'leg': 62, 'miss': 63, 'fainted': 64, 'from': 65, 'town': 66, 'athy': 67, 'one': 68, 'jeremy': 69, 'lanigan': 70, 'battered': 71, 'hadnt': 72, 'pound': 73, 'father': 74, 'died': 75, 'made': 76, 'man': 77, 'farm': 78, 'ten': 79, 'acres': 80, 'ground': 81, 'gave': 82, 'grand': 83, 'party': 84, 'who': 85, 'didnt': 86, 'forget': 87, 'come': 88, 'if': 89, 'youll': 90, 'but': 91, 'listen': 92, 'ill': 93, 'make': 94, 'your': 95, 'eyes': 96, 'glisten': 97, 'rows': 98, 'ructions': 99, 'be': 100, 'sure': 101, 'free': 102, 'invitation': 103, 'might': 104, 'ask': 105, 'minute': 106, 'both': 107, 'bees': 108, 'cask': 109, 'judy': 110, 'odaly': 111, 'little': 112, 'milliner': 113, 'wink': 114, 'give': 115, 'call': 116, 'arrived': 117, 'with': 118, 'peggy': 119, 'mcgilligan': 120, 'lashings': 121, 'punch': 122, 'wine': 123, 'ladies': 124, 'potatoes': 125, 'cakes': 126, 'bacon': 127, 'tea': 128, 'nolans': 129, 'dolans': 130, 'ogradys': 131, 'courting': 132, 'songs': 133, 'went': 134, 'plenty': 135, 'water': 136, 'harp': 137, 'once': 138, 'sounded': 139, 'taras': 140, 'hall': 141, 'sweet': 142, 'nelly': 143, 'gray': 144, 'rat': 145, 'catchers': 146, 'daughter': 147, 'singing': 148, 'together': 149, 'doing': 150, 'kinds': 151, 'nonsensical': 152, 'polkas': 153, 'room': 154, 'whirligig': 155, 'julia': 156, 'we': 157, 'banished': 158, 'nonsense': 159, 'twist': 160, 'reel': 161, 'jig': 162, 'ach': 163, 'mavrone': 164, 'how': 165, 'mad': 166, 'youd': 167, 'think': 168, 'ceiling': 169, 'would': 170, 'fall': 171, 'brooks': 172, 'academy': 173, 'learn': 174, 'nothing': 175, 'hearty': 176, 'around': 177, 'couples': 178, 'groups': 179, 'accident': 180, 'happened': 181, 'young': 182, 'terrance': 183, 'mccarthy': 184, 'right': 185, 'through': 186, 'finnertys': 187, 'hoops': 188, 'poor': 189, 'creature': 190, 'cried': 191, 'meelia': 192, 'murther': 193, 'called': 194, 'brothers': 195, 'gathered': 196, 'carmody': 197, 'swore': 198, 'hed': 199, 'go': 200, 'no': 201, 'further': 202, 'had': 203, 'satisfaction': 204, 'midst': 205, 'row': 206, 'kerrigan': 207, 'cheeks': 208, 'same': 209, 'red': 210, 'rose': 211, 'some': 212, 'lads': 213, 'declared': 214, 'painted': 215, 'took': 216, 'small': 217, 'drop': 218, 'too': 219, 'much': 220, 'suppose': 221, 'sweetheart': 222, 'ned': 223, 'morgan': 224, 'so': 225, 'powerful': 226, 'able': 227, 'saw': 228, 'fair': 229, 'colleen': 230, 'stretched': 231, 'by': 232, 'tore': 233, 'under': 234, 'table': 235, 'smashed': 236, 'chaneys': 237, 'oh': 238, 'twas': 239, 'then': 240, 'runctions': 241, 'lick': 242, 'big': 243, 'phelim': 244, 'mchugh': 245, 'replied': 246, 'introduction': 247, 'kicked': 248, 'terrible': 249, 'hullabaloo': 250, 'casey': 251, 'piper': 252, 'near': 253, 'being': 254, 'strangled': 255, 'squeezed': 256, 'pipes': 257, 'bellows': 258, 'chanters': 259, 'ribbons': 260, 'entangled': 261, 'end': 262}\n",
            "263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV97qAxGxouH",
        "colab_type": "text"
      },
      "source": [
        "So now, we have to split our sequences into our x's and our y's. To do this, let's grab the first n tokens, and make them our x's. We'll then get the last token and make it our label. Before the label becomes a y, there's one more step, and you'll see that shortly. Python makes this really easy to do with it's less syntax.\n",
        "So to get my x's, I just get all of the input sequences sliced to remove the last token. To get the labels, I get all of the input sequence sliced to keep the last token. Now, I should one-hot encode my labels as this really is a classification problem. Where given a sequence of words, I can classify from the corpus, what the next word would likely be. So to one-hot encode, I can use the contrast utility to convert a list to a categorical. I simply give it the list of labels and the number of classes which is my number of words, and it will create a one-hot encoding of the labels. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "soPGVheskaQP",
        "colab": {}
      },
      "source": [
        "input_sequences = []\n",
        "\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "# pad sequences \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# create predictors and label\n",
        "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "# one hot encode, list of labels and number of clases wich is the number of words\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pJtwVB2NbOAP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "104eb1b0-4b72-4fb7-d2f9-cd0b47b4c0d9"
      },
      "source": [
        "print(tokenizer.word_index['in'])\n",
        "print(tokenizer.word_index['the'])\n",
        "print(tokenizer.word_index['town'])\n",
        "print(tokenizer.word_index['of'])\n",
        "print(tokenizer.word_index['athy'])\n",
        "print(tokenizer.word_index['one'])\n",
        "print(tokenizer.word_index['jeremy'])\n",
        "print(tokenizer.word_index['lanigan'])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "2\n",
            "66\n",
            "8\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "49Cv68JOakwv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "952c4cb2-5b7e-4d7b-fea6-d343777830ab"
      },
      "source": [
        "print(xs[6])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  0  0  4  2 66  8 67 68 69]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iY-jwvfgbEF8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "afe4fcce-429a-47bc-dc62-f5614d50952d"
      },
      "source": [
        "print(ys[6])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wtzlUMYadhKt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "cd3e8dd5-fe57-4b98-f3f6-508b45296e6e"
      },
      "source": [
        "print(xs[5])\n",
        "print(ys[5])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  0  0  0  4  2 66  8 67 68]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H4myRpB1c4Gg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "00841228-add0-4197-b4bc-fae6c4468128"
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'and': 1, 'the': 2, 'a': 3, 'in': 4, 'all': 5, 'i': 6, 'for': 7, 'of': 8, 'lanigans': 9, 'ball': 10, 'were': 11, 'at': 12, 'to': 13, 'she': 14, 'stepped': 15, 'his': 16, 'girls': 17, 'as': 18, 'they': 19, 'til': 20, 'he': 21, 'again': 22, 'got': 23, 'boys': 24, 'round': 25, 'that': 26, 'her': 27, 'there': 28, 'three': 29, 'weeks': 30, 'up': 31, 'out': 32, 'him': 33, 'was': 34, 'spent': 35, 'learning': 36, 'new': 37, 'steps': 38, 'long': 39, 'away': 40, 'left': 41, 'friends': 42, 'relations': 43, 'when': 44, 'wall': 45, 'myself': 46, 'nice': 47, 'just': 48, 'dancing': 49, 'merry': 50, 'tipped': 51, 'me': 52, 'soon': 53, 'time': 54, 'old': 55, 'their': 56, 'them': 57, 'danced': 58, 'dublin': 59, 'an': 60, 'put': 61, 'leg': 62, 'miss': 63, 'fainted': 64, 'from': 65, 'town': 66, 'athy': 67, 'one': 68, 'jeremy': 69, 'lanigan': 70, 'battered': 71, 'hadnt': 72, 'pound': 73, 'father': 74, 'died': 75, 'made': 76, 'man': 77, 'farm': 78, 'ten': 79, 'acres': 80, 'ground': 81, 'gave': 82, 'grand': 83, 'party': 84, 'who': 85, 'didnt': 86, 'forget': 87, 'come': 88, 'if': 89, 'youll': 90, 'but': 91, 'listen': 92, 'ill': 93, 'make': 94, 'your': 95, 'eyes': 96, 'glisten': 97, 'rows': 98, 'ructions': 99, 'be': 100, 'sure': 101, 'free': 102, 'invitation': 103, 'might': 104, 'ask': 105, 'minute': 106, 'both': 107, 'bees': 108, 'cask': 109, 'judy': 110, 'odaly': 111, 'little': 112, 'milliner': 113, 'wink': 114, 'give': 115, 'call': 116, 'arrived': 117, 'with': 118, 'peggy': 119, 'mcgilligan': 120, 'lashings': 121, 'punch': 122, 'wine': 123, 'ladies': 124, 'potatoes': 125, 'cakes': 126, 'bacon': 127, 'tea': 128, 'nolans': 129, 'dolans': 130, 'ogradys': 131, 'courting': 132, 'songs': 133, 'went': 134, 'plenty': 135, 'water': 136, 'harp': 137, 'once': 138, 'sounded': 139, 'taras': 140, 'hall': 141, 'sweet': 142, 'nelly': 143, 'gray': 144, 'rat': 145, 'catchers': 146, 'daughter': 147, 'singing': 148, 'together': 149, 'doing': 150, 'kinds': 151, 'nonsensical': 152, 'polkas': 153, 'room': 154, 'whirligig': 155, 'julia': 156, 'we': 157, 'banished': 158, 'nonsense': 159, 'twist': 160, 'reel': 161, 'jig': 162, 'ach': 163, 'mavrone': 164, 'how': 165, 'mad': 166, 'youd': 167, 'think': 168, 'ceiling': 169, 'would': 170, 'fall': 171, 'brooks': 172, 'academy': 173, 'learn': 174, 'nothing': 175, 'hearty': 176, 'around': 177, 'couples': 178, 'groups': 179, 'accident': 180, 'happened': 181, 'young': 182, 'terrance': 183, 'mccarthy': 184, 'right': 185, 'through': 186, 'finnertys': 187, 'hoops': 188, 'poor': 189, 'creature': 190, 'cried': 191, 'meelia': 192, 'murther': 193, 'called': 194, 'brothers': 195, 'gathered': 196, 'carmody': 197, 'swore': 198, 'hed': 199, 'go': 200, 'no': 201, 'further': 202, 'had': 203, 'satisfaction': 204, 'midst': 205, 'row': 206, 'kerrigan': 207, 'cheeks': 208, 'same': 209, 'red': 210, 'rose': 211, 'some': 212, 'lads': 213, 'declared': 214, 'painted': 215, 'took': 216, 'small': 217, 'drop': 218, 'too': 219, 'much': 220, 'suppose': 221, 'sweetheart': 222, 'ned': 223, 'morgan': 224, 'so': 225, 'powerful': 226, 'able': 227, 'saw': 228, 'fair': 229, 'colleen': 230, 'stretched': 231, 'by': 232, 'tore': 233, 'under': 234, 'table': 235, 'smashed': 236, 'chaneys': 237, 'oh': 238, 'twas': 239, 'then': 240, 'runctions': 241, 'lick': 242, 'big': 243, 'phelim': 244, 'mchugh': 245, 'replied': 246, 'introduction': 247, 'kicked': 248, 'terrible': 249, 'hullabaloo': 250, 'casey': 251, 'piper': 252, 'near': 253, 'being': 254, 'strangled': 255, 'squeezed': 256, 'pipes': 257, 'bellows': 258, 'chanters': 259, 'ribbons': 260, 'entangled': 261, 'end': 262}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w9vH8Y59ajYL",
        "colab": {}
      },
      "source": [
        "  # lot of epochs cause very few data\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "  model.add(Bidirectional(LSTM(20)))\n",
        "  model.add(Dense(total_words, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  history = model.fit(xs, ys, epochs=500, verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3YXGelKThoTT",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "poeprYK8h-c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "70a7eeb6-504e-456a-849f-004c2bfb66fd"
      },
      "source": [
        "plot_graphs(history, 'accuracy')\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eftZFSA1Gu3",
        "colab_type": "text"
      },
      "source": [
        "So if I seeded with this text Lawrence went to Dublin, I am going to ask it for the next 100 words. What it's going to do, is for each of the next 100 words it's going to create a token lists using tokenizer text sequences of the seed text. Then that token list is going to get padded to the actual length that we want. Then that's going to be passed into the model. So we're going to predict the classes for the token list that was generated off of this seed text and then we'll get an output word from that. Then that will be used to feed into the next time round to predict again to get another model. So when we start with Lawrence went to Dublin, it'll get us another word and that phrase will generate another word and that phrase will generate another word etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6Vc6PHgxa6Hm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "463312bb-caa3-48a9-a008-8f114266d1e2"
      },
      "source": [
        "seed_text = \"Laurence went to dublin\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-13-40ff8cb1a6a1>:7: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "Laurence went to dublin til saw got a phelim mchugh mchugh mchugh he fall fainted mavrone away mavrone entangled mad at me mavrone a kerrigan fainted a fainted mavrone me mavrone entangled mavrone dublin relations hall fainted entangled around hearty creature rose fainted and fainted entangled and fainted and red and and girls hearty were hearty hearty might fainted entangled up entangled entangled entangled punch mavrone entangled entangled hall hall hall eyes glisten hearty he fall fainted mavrone entangled me mavrone mavrone entangled mavrone entangled fainted entangled entangled round polkas punch til hall hall fainted me mavrone mavrone entangled me mavrone mavrone as rose\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgDzoyB_1V3u",
        "colab_type": "text"
      },
      "source": [
        "So if I print that out, we'll get something like this. Lawrence went to Dublin a twist of a reel and a jig, jig gathered gathered them long new weeks i spent up jig Dublin might ask ask ask mother asks jig man again. We can see what's actually happening here is that in the beginning it kind of looks pretty good. It's beginning to make sense. But of course because our body of texts is pretty small and each prediction is a probability. So after the words Lawrence went to Dublin, the most probable word that would come next is A, but of course it's not 100 percent certainty. It's a probability and then the probability of the next probable word after Lawrence went to Dublin A would be twist and keep going twist over reel and a jig. But as you can see then, as you get further and further and further then the probabilities are decreasing and the quality of the prediction as a result goes down. So you end up with for example repeated words like Jake Jake, gathered gathered. It's kind of fun if we take a look at some of the words in the song, so we could see how they would deal with the prediction. So for example, when we say Lawrence went to Dublin, a twist of a reel. So let's take a look at a twist of a reel. If we go back to the original song and the text of the original song see if those words actually exist."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9aa5WJj0w9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}